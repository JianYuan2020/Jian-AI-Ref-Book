
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network (Classification) &#8212; Jian AI Ref 0.1 documentation</title>
    <link rel="stylesheet" href="../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Support Vector Machines" href="support_vector_machines.html" />
    <link rel="prev" title="Model Representation I" href="nn_model_representation.html" /> 
  </head><body>
    <div class="header-wrapper" role="banner">
      <div class="header">
        <div class="headertitle"><a
          href="../index.html">Jian AI Ref 0.1 documentation</a></div>
        <div class="rel" role="navigation" aria-label="related navigation">
          <a href="nn_model_representation.html" title="Model Representation I"
             accesskey="P">previous</a> |
          <a href="support_vector_machines.html" title="Support Vector Machines"
             accesskey="N">next</a> |
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="neural-network-classification">
<h1>Neural Network (Classification)<a class="headerlink" href="#neural-network-classification" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><ul class="simple">
<li><p>Training set, <span class="math notranslate nohighlight">\(m\)</span> examples: <span class="math notranslate nohighlight">\({ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}) }\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(L\)</span> = total no. of layers in network</p></li>
<li><p><span class="math notranslate nohighlight">\(s_{l}\)</span> = no. of units (not counting bias unit) in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
</ul>
<dl class="simple">
<dt>Binary Classification:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y^{(i)} = 0\)</span> or <span class="math notranslate nohighlight">\(1 \in \mathbb {R}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(1\)</span> output unit</p></li>
</ul>
</dd>
<dt>Multi-class Classification (K classes):</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y^{(i)} \in \mathbb {R^{K}}\)</span> E.g. <span class="math notranslate nohighlight">\({\begin{bmatrix}1\\0\\0\\0\end{bmatrix}}\)</span>, <span class="math notranslate nohighlight">\({\begin{bmatrix}0\\1\\0\\0\end{bmatrix}}\)</span>, <span class="math notranslate nohighlight">\({\begin{bmatrix}0\\0\\1\\0\end{bmatrix}}\)</span>, <span class="math notranslate nohighlight">\({\begin{bmatrix}0\\0\\0\\1\end{bmatrix}}\)</span></p></li>
<li><p>Representing pedestrian, car, motorcycle, and truck respectively</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span> output units</p></li>
</ul>
</dd>
</dl>
</div></blockquote>
<div class="section" id="cost-function">
<h2>Cost Function<a class="headerlink" href="#cost-function" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Logistic Regression:</p>
<p><span class="math notranslate nohighlight">\(J(\theta) = - \frac{1}{m} [ \sum_{i=1}^{m} y^{(i)} \log h_\theta (x^{(i)}) + (1 - y^{(i)}) \log(1 - h_\theta (x^{(i)})) ] +
\frac{\lambda}{2m} \sum_{j=1}^{n} \theta_{j}^2\)</span></p>
<blockquote>
<div><ul class="simple">
<li><p>Exclude <span class="math notranslate nohighlight">\(\theta_{0}\)</span> for regularization</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>Neural Network:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h_\Theta (x) \in \mathbb {R^{K}}\)</span> and <span class="math notranslate nohighlight">\((h_\Theta (x))_{k} = k^{th}\)</span> output</p></li>
</ul>
</dd>
</dl>
<p><span class="math notranslate nohighlight">\(J(\Theta) = - \frac{1}{m} [ \sum_{i=1}^{m} \sum_{k=1}^{K} y^{(i)}_{k} \log(h_\Theta (x^{(i)}))_{k} + (1 - y^{(i)}_{k}) \log(1 - (h_\Theta (x^{(i)}))_{k}) ] +
\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l+1}} (\Theta_{ji}^{(l)})^2\)</span></p>
</div></blockquote>
</div>
<div class="section" id="backpropagation-algorithm">
<h2>Backpropagation Algorithm<a class="headerlink" href="#backpropagation-algorithm" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><dl class="simple">
<dt>Gradient Computation</dt><dd><ul class="simple">
<li><p>Cost function <span class="math notranslate nohighlight">\(J(\Theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\min_{\Theta} J(\Theta)\)</span></p></li>
</ul>
</dd>
<dt>Need code to compute:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J(\Theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{ji}^{(l)}} J(\Theta)\)</span>, <span class="math notranslate nohighlight">\(\Theta_{ji}^{(l)} \in \mathbb {R}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Given one training example (<span class="math notranslate nohighlight">\(x, y\)</span>):</p>
<dl class="simple">
<dt>Forward Propagation:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a^{(1)} = x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(z^{(2)} = \Theta^{(1)} a^{(1)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a^{(2)} = g(z^{(2)})\)</span>, (add <span class="math notranslate nohighlight">\(a_{0}^{(2)} = 1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(z^{(3)} = \Theta^{(2)} a^{(2)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a^{(3)} = g(z^{(3)})\)</span>, (add <span class="math notranslate nohighlight">\(a_{0}^{(3)} = 1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(z^{(4)} = \Theta^{(3)} a^{(3)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a^{(4)} = h_\Theta (x) = g(z^{(4)})\)</span></p></li>
</ul>
</dd>
</dl>
<p>Gradient computation: Backpropagation algorithm:</p>
<p>Intuition: <span class="math notranslate nohighlight">\(\delta_{j}^{(l)}\)</span> = “error” of node <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p>
<dl class="simple">
<dt>For each output unit (layer <span class="math notranslate nohighlight">\(L = 4\)</span>)</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{j}^{(4)} = a_{j}^{(4)} - y_{j}\)</span>, <span class="math notranslate nohighlight">\(a_{j}^{(4)} = (h_\theta (x))_{j}\)</span></p></li>
</ul>
</dd>
<dt>Or</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta^{(4)} = a^{(4)} - y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta^{(3)} = (\Theta^{(3)})^{T} \delta^{(4)} .* g'(z^{(3)})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta^{(2)} = (\Theta^{(2)})^{T} \delta^{(3)} .* g'(z^{(2)})\)</span></p></li>
<li><p>No <span class="math notranslate nohighlight">\(\delta^{(1)}\)</span></p></li>
</ul>
</dd>
</dl>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{ji}^{(l)}} J(\Theta) = a_{j}^{(l)} \delta_{i}^{(l+1)}\)</span> (ignore <span class="math notranslate nohighlight">\(\lambda\)</span> for now, set <span class="math notranslate nohighlight">\(\lambda = 0\)</span>)</p></li>
</ul>
<dl class="simple">
<dt>Backpropagation Algorithm:</dt><dd><ul class="simple">
<li><p>Training set <span class="math notranslate nohighlight">\({ (x^{(1)}, y^{(1)}), ..., (x^{(m)}, y^{(m)}) }\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\Delta^{(l)}_{ij} = 0\)</span> (for all <span class="math notranslate nohighlight">\(l, i, j\)</span>); (used to update <span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{ji}^{(l)}} J(\Theta)\)</span>)</p></li>
<li><dl class="simple">
<dt><span class="math notranslate nohighlight">\(for\)</span> <span class="math notranslate nohighlight">\(i = 1:m\)</span> &lt;- <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span></dt><dd><ul>
<li><p>Set <span class="math notranslate nohighlight">\(a^{(1)} = x^{(i)}\)</span></p></li>
<li><p>Perform forward propagation to compute <span class="math notranslate nohighlight">\(a^{(1)}\)</span> (for all <span class="math notranslate nohighlight">\(l = 2, 3, ..., L\)</span>)</p></li>
<li><p>Using <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, compute <span class="math notranslate nohighlight">\(\delta^{(L)} = a^{(L)} - y^{(i)}\)</span></p></li>
<li><p>Perform backward propagation to compute <span class="math notranslate nohighlight">\(\delta^{(L-1)}, \delta^{(L-2)}, ..., \delta^{(2)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta^{(l)}_{ij} = \Delta^{(l)}_{ij} + a_{j}^{(l)} \delta_{i}^{(l+1)}\)</span></p></li>
<li><p>Or <span class="math notranslate nohighlight">\(\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^{T}\)</span></p></li>
</ul>
</dd>
</dl>
</li>
<li><p><span class="math notranslate nohighlight">\(D^{(l)}_{ij} = \frac{1}{m} \Delta^{(l)}_{ij}\)</span>; (for <span class="math notranslate nohighlight">\(j = 0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(D^{(l)}_{ij} = \frac{1}{m} \Delta^{(l)}_{ij} + \lambda \Theta_{ij}^{(l)}\)</span>; (for <span class="math notranslate nohighlight">\(j \neq 0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{ji}^{(l)}} J(\Theta) = D^{(l)}_{ij}\)</span></p></li>
</ul>
</dd>
</dl>
</div></blockquote>
</div>
<div class="section" id="backpropagation-intuition">
<h2>Backpropagation Intuition<a class="headerlink" href="#backpropagation-intuition" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>What is backpropagation doing?</dt><dd><p><span class="math notranslate nohighlight">\(J(\Theta) = - \frac{1}{m} [ \sum_{i=1}^{m} y^{(i)} \log(h_\Theta (x^{(i)})) + (1 - y^{(i)}) \log(1 - (h_\Theta (x^{(i)}))) ] +
\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l+1}} (\Theta_{ji}^{(l)})^2\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Focusing on a single example <span class="math notranslate nohighlight">\(x^{(i)}, y^{(i)}\)</span>, the case of <span class="math notranslate nohighlight">\(1\)</span> output unit, and ignoring regularization (<span class="math notranslate nohighlight">\(\lambda = 0\)</span>):</dt><dd><p><span class="math notranslate nohighlight">\(cost(i) = y^{(i)} \log(h_\Theta (x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\Theta (x^{(i)}))\)</span></p>
</dd>
</dl>
</li>
<li><p>(Think of <span class="math notranslate nohighlight">\(cost(i)\)</span> ~ <span class="math notranslate nohighlight">\((h_\Theta (x^{(i)}) - y^{(i)})^2\)</span>)</p></li>
<li><p>I.e. how well is the network doing on example <span class="math notranslate nohighlight">\(i\)</span>?</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{j}^{(l)}\)</span> = “error” of cost for <span class="math notranslate nohighlight">\(a_{j}^{(l)}\)</span> (unit <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>)</p></li>
<li><dl class="simple">
<dt>Formally, <span class="math notranslate nohighlight">\(\delta_{j}^{(l)} = \frac{\partial }{\partial z_{j}^{(l)}} cost(i)\)</span> (for <span class="math notranslate nohighlight">\(j \geq 0\)</span>), where</dt><dd><p><span class="math notranslate nohighlight">\(cost(i) = y^{(i)} \log(h_\Theta (x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\Theta (x^{(i)}))\)</span></p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="implementation-note-unrolling-parameters">
<h2>Implementation Note: Unrolling Parameters:<a class="headerlink" href="#implementation-note-unrolling-parameters" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/unroll_into_vectors1.png"><img alt="../_images/unroll_into_vectors1.png" class="align-center" src="../_images/unroll_into_vectors1.png" style="width: 571.1999999999999px; height: 253.39999999999998px;" /></a>
<a class="reference internal image-reference" href="../_images/unroll_into_vectors2.png"><img alt="../_images/unroll_into_vectors2.png" class="align-center" src="../_images/unroll_into_vectors2.png" style="width: 570.5px; height: 273.0px;" /></a>
<a class="reference internal image-reference" href="../_images/unroll_into_vectors3.png"><img alt="../_images/unroll_into_vectors3.png" class="align-center" src="../_images/unroll_into_vectors3.png" style="width: 568.4px; height: 271.59999999999997px;" /></a>
</div>
<div class="section" id="gradient-checking">
<h2>Gradient Checking<a class="headerlink" href="#gradient-checking" title="Permalink to this headline">¶</a></h2>
<div class="section" id="numerical-estimation-of-gradients">
<h3>Numerical Estimation of Gradients<a class="headerlink" href="#numerical-estimation-of-gradients" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Implement: <span class="math notranslate nohighlight">\(\frac{\partial }{\partial \theta} J(\theta) \approx gradApprox = \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon}\)</span>, <span class="math notranslate nohighlight">\(\theta \in \mathbb {R}, \epsilon = 10^{-4}\)</span></p>
<dl class="simple">
<dt>Parameter vector <span class="math notranslate nohighlight">\(\theta\)</span>:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta = \theta_{1}, \theta_{2}, ..., \theta_{n}; \theta \in \mathbb {R^{n}}\)</span> (E.g. <span class="math notranslate nohighlight">\(\theta\)</span> is “unrolled” version of <span class="math notranslate nohighlight">\(\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial }{\partial \theta_{1}} J(\theta) = \frac{J(\theta_{1} + \epsilon, \theta_{2}, ..., \theta_{n}) - J(\theta_{1} - \epsilon, \theta_{2}, ..., \theta_{n})}{2\epsilon}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial }{\partial \theta_{2}} J(\theta) = \frac{J(\theta_{1}, \theta_{2} + \epsilon, ..., \theta_{n}) - J(\theta_{1}, \theta_{2} - \epsilon, ..., \theta_{n})}{2\epsilon}\)</span></p></li>
<li><p>—</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{\partial }{\partial \theta_{n}} J(\theta) = \frac{J(\theta_{1}, \theta_{2}, ..., \theta_{n} + \epsilon) - J(\theta_{1}, \theta_{2}, ..., \theta_{n} - \epsilon)}{2\epsilon}\)</span></p></li>
</ul>
</dd>
</dl>
</div></blockquote>
</div>
<div class="section" id="octave-code">
<h3>Octave Code<a class="headerlink" href="#octave-code" title="Permalink to this headline">¶</a></h3>
<div class="highlight-octave notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">n</span><span class="p">,</span>
        <span class="n">thetaPlus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
        <span class="n">thetaPlus</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+=</span> <span class="n">EPSILON</span><span class="p">;</span>
        <span class="n">thetaMinus</span> <span class="p">=</span> <span class="n">theta</span><span class="p">;</span>
        <span class="n">thetaMinus</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">-=</span> <span class="n">EPSILON</span><span class="p">;</span>
        <span class="n">gradApprox</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="p">=</span> <span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">thetaPlus</span><span class="p">)</span> <span class="o">-</span> <span class="n">J</span><span class="p">(</span><span class="n">thetaMinus</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">EPSILON</span><span class="p">);</span>
<span class="k">end</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="checking">
<h3>Checking<a class="headerlink" href="#checking" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Check that <span class="math notranslate nohighlight">\(gradApprox \approx DVec\)</span> (from backprop)</p>
</div></blockquote>
</div>
<div class="section" id="implementation-note">
<h3>Implementation Note:<a class="headerlink" href="#implementation-note" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>Implement backprop to compute DVec (unrolled <span class="math notranslate nohighlight">\(D^{(1)}, D^{(2)}, D^{(3)}\)</span>)</p></li>
<li><p>Implement numerical gradient check to compute gradApprox</p></li>
<li><p>Make sure they give similar values</p></li>
<li><p>Turn off gradient checking. Using backprop code for learning.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="important">
<h3>Important:<a class="headerlink" href="#important" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>Be sure to disable your gradient checking code before training your classifier. If you run numerical
gradient computation on every iteration of gradient descent (or in the inner loop of costFunction(…))
your code will be very slow.</p></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="random-initialization-label">
<h2><a class="reference internal" href="../_appendix/random_initialization.html#random-initialization-label"><span class="std std-ref">Random Initialization</span></a><a class="headerlink" href="#random-initialization-label" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="putting-it-together">
<h2>Putting It Together<a class="headerlink" href="#putting-it-together" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pick-a-network-architecture-connectivity-pattern-between-neurons">
<h3>Pick a Network Architecture (connectivity pattern between neurons)<a class="headerlink" href="#pick-a-network-architecture-connectivity-pattern-between-neurons" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>No. of input units: Dimension of features <span class="math notranslate nohighlight">\(x^{(i)}\)</span></p></li>
<li><p>No. output units: Number of classes <span class="math notranslate nohighlight">\(y^{(i)} \in \mathbb {R^{K}}\)</span></p></li>
<li><p>Reasonable default: <span class="math notranslate nohighlight">\(1\)</span> hidden layer, or if <span class="math notranslate nohighlight">\(&gt; 1\)</span> hidden layer, have same no. of hidden units
in every layer or not (usually the more the better)</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="training-a-neural-network">
<h3>Training a Neural Network<a class="headerlink" href="#training-a-neural-network" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ol class="arabic simple">
<li><p>Randomly initialize weights</p></li>
<li><p>Implement forward propagation to get <span class="math notranslate nohighlight">\(h_{\Theta}(x^{(i)})\)</span> for any <span class="math notranslate nohighlight">\(x^{(i)}\)</span></p></li>
<li><p>Implement code to compute cost function <span class="math notranslate nohighlight">\(J(\Theta)\)</span></p></li>
<li><p>Implement backprop to compute partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{jk}^{(l)}} J(\Theta)\)</span></p></li>
</ol>
<dl>
<dt><span class="math notranslate nohighlight">\(for\)</span> <span class="math notranslate nohighlight">\(i = 1:m\)</span></dt><dd><p>Perform forward propagation and backpropagation using example <span class="math notranslate nohighlight">\((x^{(i)}, y^{(i)})\)</span></p>
<p>(Get activations <span class="math notranslate nohighlight">\(a^{(l)}\)</span> and delta terms <span class="math notranslate nohighlight">\(\delta^{(l)}\)</span> for <span class="math notranslate nohighlight">\(l = 2, ..., L\)</span>)</p>
<p><span class="math notranslate nohighlight">\(\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^{T}\)</span></p>
</dd>
</dl>
<p>Compute <span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{jk}^{(l)}} J(\Theta)\)</span></p>
<ol class="arabic simple" start="5">
<li><p>Use gradient checking to compare <span class="math notranslate nohighlight">\(\frac{\partial }{\partial \Theta_{jk}^{(l)}} J(\Theta)\)</span> computed
using backpropagation vs. using numerical estimate of gradient of <span class="math notranslate nohighlight">\(J(\Theta)\)</span>. Then disable
gradient checking code.</p></li>
<li><p>Use gradient descent or advanced optimization method with backpropagation to try to minimize
<span class="math notranslate nohighlight">\(J(\Theta)\)</span> as a function of parameters <span class="math notranslate nohighlight">\(\Theta\)</span></p></li>
</ol>
</div></blockquote>
</div>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          
          <h3>Table of Contents</h3>
          <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning.html">Machine Learning</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../supervised_learning.html">Supervised learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear_regression.html">Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="logistic_regression.html">Logistic Regression</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="neural_networks.html">Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="support_vector_machines.html">Support Vector Machines</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised_learning.html">Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../special_apps_topics.html">Special Applications/Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advice.html">Advice on Building a ML System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorflow.html">TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>

          <div role="search">
            <h3 style="margin-top: 1.5em;">Search</h3>
            <form class="search" action="../search.html" method="get">
                <input type="text" name="q" />
                <input type="submit" value="Go" />
            </form>
          </div>

        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <div role="navigation" aria-label="related navigaton">
            <a href="nn_model_representation.html" title="Model Representation I"
              >previous</a> |
            <a href="support_vector_machines.html" title="Support Vector Machines"
              >next</a> |
            <a href="../genindex.html" title="General Index"
              >index</a>
          </div>
          <div role="note" aria-label="source link">
              <br/>
              <a href="../_sources/_supervised/nn_classification.rst.txt"
                rel="nofollow">Show Source</a>
          </div>
        </div>

        <div class="right">
          
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Jian Yuan.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.2.1.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>